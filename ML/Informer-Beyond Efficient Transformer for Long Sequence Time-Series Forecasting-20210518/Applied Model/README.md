# Applied Model: Informer Production Deployment

**Version:** 1.0.0  
**Date:** October 14, 2025  
**Status:** Production Ready

---

## üìã Overview

This folder contains **production-ready specifications and templates** for deploying the Informer time series forecasting model with SQL database integration.

### What's Included

| File | Purpose | Key Features |
|------|---------|--------------|
| `model_specs.py` | Complete model configuration system | 120+ parameters, type-safe, presets for common use cases |
| `config_matrix.csv` | Parameter reference guide | All parameters with definitions, scopes, and examples |
| `sql_data_pipeline.py` | SQL data extraction pipeline | Optimized queries, connection pooling, validation |
| `deployment_template.py` | End-to-end deployment template | Complete pipeline from data to API |
| `README.md` | This file | Documentation and usage guide |

---

## üöÄ Quick Start

### 1. Install Dependencies

```bash
pip install torch pandas numpy sqlalchemy psycopg2-binary pyarrow scikit-learn
```

### 2. Set Environment Variables

```bash
export DB_HOST="your-database-host.com"
export DB_PORT="5432"
export DB_NAME="timeseries_db"
export DB_USER="your_username"
export DB_PASSWORD="your_password"
export MLFLOW_URI="http://mlflow-server:5000"
```

### 3. Create Configuration

```python
from model_specs import ConfigPresets

# Start with a preset
config = ConfigPresets.energy_forecasting()

# Customize for your use case
config.data_source.host = "your-db-host.com"
config.data_schema.table_name = "your_table"
config.data_schema.target_column = "your_target"

# Save configuration
config.to_json("my_config.json")
```

### 4. Extract Data from SQL

```python
from sql_data_pipeline import SQLTimeSeriesPipeline

# Create pipeline
pipeline = SQLTimeSeriesPipeline(config)

# Extract data
df = pipeline.run()

# Save for faster iteration
pipeline.save_to_parquet("data/extracted_data.parquet")
```

### 5. Run Complete Pipeline

```bash
python deployment_template.py full-pipeline --config my_config.json
```

---

## üìä Configuration System

### Configuration Hierarchy

```
InformerConfig (Master)
‚îú‚îÄ‚îÄ DataSourceConfig (Database connection)
‚îú‚îÄ‚îÄ DataTableSchema (Table structure)
‚îú‚îÄ‚îÄ DataFilterConfig (Query filters)
‚îú‚îÄ‚îÄ DataPreprocessingConfig (Cleaning)
‚îú‚îÄ‚îÄ NormalizationConfig (Scaling)
‚îú‚îÄ‚îÄ InformerArchitectureConfig (Model architecture)
‚îú‚îÄ‚îÄ OptimizationConfig (Training optimization)
‚îú‚îÄ‚îÄ TrainingConfig (Training loop)
‚îú‚îÄ‚îÄ DataSplitConfig (Train/val/test split)
‚îú‚îÄ‚îÄ InferenceConfig (Prediction settings)
‚îú‚îÄ‚îÄ DeploymentConfig (Production deployment)
‚îî‚îÄ‚îÄ ExperimentConfig (MLOps tracking)
```

### Available Presets

| Preset | Use Case | seq_len | pred_len | Frequency |
|--------|----------|---------|----------|-----------|
| `small_dataset()` | <100K samples | 96 | 24 | Any |
| `medium_dataset()` | 100K-1M samples | 96 | 24 | Any |
| `large_dataset()` | >1M samples | 96 | 24 | Any |
| `energy_forecasting()` | Electricity load | 168 | 24 | Hourly |
| `financial_forecasting()` | Stock prices | 252 | 20 | Daily |
| `weather_forecasting()` | Temperature | 144 | 48 | 10-min |
| `traffic_forecasting()` | Traffic flow | 96 | 12 | 15-min |

### Example: Custom Configuration

```python
from model_specs import InformerConfig

# Start from scratch
config = InformerConfig()

# Configure database
config.data_source.database_type = "postgresql"
config.data_source.host = "prod-db.company.com"
config.data_source.database = "analytics"

# Configure data
config.data_schema.table_name = "sensor_readings"
config.data_schema.timestamp_column = "measured_at"
config.data_schema.target_column = "temperature"
config.data_schema.value_columns = ["temp", "humidity", "pressure"]

# Configure model
config.architecture.seq_len = 168  # 1 week lookback
config.architecture.pred_len = 24  # 1 day forecast
config.architecture.d_model = 512
config.architecture.n_heads = 8

# Configure training
config.training.batch_size = 32
config.training.num_epochs = 10
config.training.use_gpu = True

# Validate and save
config.validate()
config.to_json("sensor_config.json")
```

---

## üóÑÔ∏è SQL Database Integration

### Supported Databases

- ‚úÖ PostgreSQL
- ‚úÖ MySQL
- ‚úÖ Google BigQuery
- ‚úÖ Snowflake

### Connection Configuration

```python
from model_specs import DataSourceConfig

db_config = DataSourceConfig(
    database_type="postgresql",
    host="your-host.com",
    port=5432,
    database="timeseries_db",
    username="readonly_user",
    password="secure_password",
    use_ssl=True,
    connection_pool_size=5
)
```

### Query Optimization Features

1. **Automatic WHERE clause generation** from filters
2. **Connection pooling** for concurrent queries
3. **Chunk-based loading** for large datasets
4. **Time-based aggregation** for downsampling
5. **Window functions** for feature engineering
6. **Multi-entity support** for batch forecasting

### Example: Extract Data with Filters

```python
from model_specs import InformerConfig
from sql_data_pipeline import SQLTimeSeriesPipeline

config = InformerConfig()

# Configure filters
config.data_filter.start_date = "2023-01-01"
config.data_filter.end_date = "2024-01-01"
config.data_filter.exclude_nulls = True
config.data_filter.min_value = 0.0
config.data_filter.max_value = 1000.0

# Extract
pipeline = SQLTimeSeriesPipeline(config)
df = pipeline.run()
```

### Example: Multi-Entity Forecasting

```python
# Configure for multiple sensors/locations
config.data_schema.entity_column = "sensor_id"
config.data_filter.entity_ids = ["SENSOR_001", "SENSOR_002", "SENSOR_003"]

# Extract all entities
pipeline = SQLTimeSeriesPipeline(config)
df = pipeline.run()

# Or extract one entity at a time
for entity_id in config.data_filter.entity_ids:
    entity_df = pipeline.extractor.extract_by_entity(entity_id)
    # Process entity_df...
```

---

## üîß Configuration Reference

### Critical Parameters

| Parameter | Location | Default | Description |
|-----------|----------|---------|-------------|
| `seq_len` | architecture | 96 | Input sequence length (lookback window) |
| `pred_len` | architecture | 24 | Prediction horizon (forecast length) |
| `d_model` | architecture | 512 | Model embedding dimension |
| `n_heads` | architecture | 8 | Number of attention heads |
| `e_layers` | architecture | 2 | Number of encoder layers |
| `batch_size` | training | 32 | Training batch size |
| `learning_rate` | optimization | 1e-4 | Initial learning rate |

### Data Configuration

```python
# Table schema
config.data_schema.table_name = "time_series_data"
config.data_schema.timestamp_column = "timestamp"
config.data_schema.target_column = "target"
config.data_schema.value_columns = ["feature_1", "feature_2", ...]

# Filters
config.data_filter.start_date = "2023-01-01"
config.data_filter.end_date = "2024-01-01"
config.data_filter.entity_ids = ["ID1", "ID2"]  # Optional

# Preprocessing
config.preprocessing.missing_value_strategy = "forward_fill"
config.preprocessing.outlier_treatment = "clip"
config.preprocessing.create_time_features = True

# Normalization
config.normalization.method = "standard"  # standard, minmax, robust
config.normalization.per_feature = True
config.normalization.fit_on_train_only = True  # CRITICAL!
```

### Model Architecture

```python
# Input/Output
config.architecture.enc_in = 7   # Number of input features
config.architecture.dec_in = 7   # Decoder input features
config.architecture.c_out = 1    # Number of outputs

# Sequence lengths
config.architecture.seq_len = 96      # Lookback
config.architecture.label_len = 48    # Start token
config.architecture.pred_len = 24     # Forecast

# Model size
config.architecture.d_model = 512     # Model dimension
config.architecture.d_ff = 2048       # FFN dimension
config.architecture.n_heads = 8       # Attention heads

# Layers
config.architecture.e_layers = 2      # Encoder layers
config.architecture.d_layers = 1      # Decoder layers

# ProbSparse attention
config.architecture.factor = 5        # Sampling factor
config.architecture.distil = True     # Use distilling
```

### Training Configuration

```python
# Training loop
config.training.batch_size = 32
config.training.num_epochs = 10
config.training.use_early_stopping = True
config.training.patience = 3

# Hardware
config.training.use_gpu = True
config.training.gpu_ids = [0, 1, 2]
config.training.use_multi_gpu = True
config.training.num_workers = 4

# Checkpointing
config.training.save_checkpoints = True
config.training.checkpoint_dir = "./checkpoints/"
config.training.save_best_only = True

# Optimization
config.optimization.optimizer = "adam"
config.optimization.learning_rate = 1e-4
config.optimization.use_lr_scheduler = True
config.optimization.use_mixed_precision = True
config.optimization.use_grad_clip = True
```

---

## üìà Usage Examples

### Example 1: Energy Load Forecasting

```python
from model_specs import ConfigPresets
from sql_data_pipeline import SQLTimeSeriesPipeline

# Use energy preset
config = ConfigPresets.energy_forecasting()

# Customize
config.data_source.host = "energy-db.company.com"
config.data_schema.table_name = "grid_loads"
config.data_schema.target_column = "load_mw"
config.data_schema.value_columns = [
    "temperature", "humidity", "hour_of_day", 
    "day_of_week", "is_holiday", "load_mw"
]

# Extract and preprocess
pipeline = SQLTimeSeriesPipeline(config)
df = pipeline.run()

# Train model (implement in deployment_template.py)
# train_model(df, config)
```

### Example 2: Stock Price Prediction

```python
config = ConfigPresets.financial_forecasting()

config.data_schema.table_name = "stock_ohlcv"
config.data_schema.target_column = "close_price"
config.data_schema.value_columns = [
    "open", "high", "low", "close", "volume", "vwap"
]

# Financial data often has outliers - use robust scaling
config.normalization.method = "robust"

# Extract data
pipeline = SQLTimeSeriesPipeline(config)
df = pipeline.run()
```

### Example 3: Weather Forecasting

```python
config = ConfigPresets.weather_forecasting()

config.data_schema.table_name = "weather_stations"
config.data_schema.target_column = "temperature"
config.data_schema.value_columns = [
    "temperature", "humidity", "pressure", 
    "wind_speed", "precipitation"
]

# 10-minute data
config.architecture.freq = "t"

# Multi-station forecasting
config.data_schema.entity_column = "station_id"
config.data_filter.entity_ids = ["STATION_001", "STATION_002"]

pipeline = SQLTimeSeriesPipeline(config)
df = pipeline.run()
```

### Example 4: Traffic Flow Prediction

```python
config = ConfigPresets.traffic_forecasting()

config.data_schema.table_name = "traffic_sensors"
config.data_schema.target_column = "flow_rate"
config.data_schema.value_columns = [
    "flow_rate", "occupancy", "speed", "is_rush_hour"
]

# 15-minute intervals
config.architecture.freq = "t"

# Configure for real-time inference
config.inference.inference_batch_size = 1
config.deployment.enable_monitoring = True

pipeline = SQLTimeSeriesPipeline(config)
df = pipeline.run()
```

---

## üèóÔ∏è Architecture Specifications

### Model Sizes

| Size | d_model | d_ff | n_heads | e_layers | Parameters | Use Case |
|------|---------|------|---------|----------|------------|----------|
| Small | 256 | 1024 | 4 | 2 | ~5M | Testing, small datasets |
| Medium | 512 | 2048 | 8 | 2 | ~20M | Most production use cases |
| Large | 512 | 2048 | 8 | 3 | ~30M | Complex patterns, large data |
| XLarge | 768 | 3072 | 12 | 4 | ~60M | Research, maximum accuracy |

### Complexity Analysis

**Vanilla Transformer:**
- Time: O(L¬≤ ¬∑ d)
- Space: O(L¬≤ ¬∑ E)

**Informer:**
- Time: O(L log L ¬∑ d)
- Space: O(L (log L + E))

**Example:** For L=512, d=512
- Vanilla: ~134M operations
- Informer: ~23M operations
- **Speedup: ~5.8x**

### Memory Requirements

| seq_len | batch_size | d_model | GPU Memory (GB) |
|---------|------------|---------|-----------------|
| 96 | 32 | 512 | ~2.5 |
| 168 | 32 | 512 | ~3.5 |
| 336 | 32 | 512 | ~5.0 |
| 512 | 32 | 512 | ~6.5 |
| 1024 | 16 | 512 | ~8.0 |
| 2048 | 8 | 512 | ~10.5 |

---

## üì¶ Deployment

### Docker Deployment

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "deployment_template.py", "deploy", "--config", "production_config.json"]
```

### Kubernetes Deployment

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: informer-forecasting
spec:
  replicas: 3
  selector:
    matchLabels:
      app: informer
  template:
    metadata:
      labels:
        app: informer
    spec:
      containers:
      - name: informer
        image: your-registry/informer:latest
        ports:
        - containerPort: 8000
        env:
        - name: DB_HOST
          valueFrom:
            secretKeyRef:
              name: db-secrets
              key: host
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
            nvidia.com/gpu: "1"
          limits:
            memory: "8Gi"
            cpu: "4000m"
            nvidia.com/gpu: "1"
```

### API Server Template

```python
from fastapi import FastAPI
from pydantic import BaseModel
import torch

app = FastAPI(title="Informer Forecasting API")

# Load model at startup
model = None
scaler = None

@app.on_event("startup")
async def load_model():
    global model, scaler
    # Load model and scaler
    pass

class PredictionRequest(BaseModel):
    entity_id: str
    timestamp: str
    features: List[float]

@app.post("/predict")
async def predict(request: PredictionRequest):
    # Preprocess
    # Inference
    # Postprocess
    return {"predictions": predictions.tolist()}

@app.get("/health")
async def health():
    return {"status": "healthy"}
```

---

## üß™ Testing

### Unit Tests

```python
# test_config.py
def test_config_validation():
    config = InformerConfig()
    config.architecture.seq_len = 96
    config.architecture.pred_len = 24
    config.validate()  # Should pass

def test_config_presets():
    for preset_name in ['small', 'medium', 'large']:
        config = create_config_from_preset(preset_name)
        assert config.architecture.d_model > 0
```

### Integration Tests

```python
# test_pipeline.py
def test_sql_extraction():
    config = ConfigPresets.energy_forecasting()
    pipeline = SQLTimeSeriesPipeline(config)
    df = pipeline.run()
    assert len(df) > 0
    assert config.data_schema.target_column in df.columns
```

---

## üìä Monitoring

### Metrics to Track

**Training:**
- Loss (MSE/MAE)
- Validation metrics
- Learning rate
- Gradient norms
- Training time per epoch

**Inference:**
- Latency (p50, p95, p99)
- Throughput (predictions/sec)
- Error rate
- Cache hit rate

**Data Quality:**
- Missing value percentage
- Outlier count
- Distribution shift
- Data freshness

### Logging Configuration

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('informer.log'),
        logging.StreamHandler()
    ]
)
```

---

## üîí Security Best Practices

1. **Never hardcode credentials** - use environment variables
2. **Use SSL/TLS** for database connections
3. **Implement API authentication** (JWT, OAuth)
4. **Rate limiting** on API endpoints
5. **Input validation** on all user inputs
6. **Audit logging** for predictions
7. **Model versioning** for rollback capability
8. **Regular security updates** for dependencies

---

## üìö Additional Resources

### Documentation
- **MATH_BREAKDOWN.txt** - Mathematical foundations
- **RESEARCH_BREAKDOWN.txt** - Research insights and applications
- **IMPLEMENTATION_SPEC.md** - Detailed implementation guide
- **DATA_ENGINEERING_SPEC.md** - Data preprocessing guide

### Papers
- Informer: [arXiv:2012.07436](https://arxiv.org/abs/2012.07436)
- Attention Is All You Need: [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

### Code Repositories
- Official Informer: https://github.com/zhouhaoyi/Informer2020
- This implementation: [Your repository]

---

## ü§ù Support

For questions or issues:

1. Check `config_matrix.csv` for parameter definitions
2. Review example code in this README
3. See `deployment_template.py` for complete pipeline
4. Contact: engineering@your-company.com

---

## üìù License

Copyright (c) 2025 Your Company. All rights reserved.

---

## üîÑ Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0.0 | 2025-10-14 | Initial release |

---

**Built with ‚ù§Ô∏è by the ML Engineering Team**

