# Applied Model: Informer Production Deployment

**Version:** 1.0.0  
**Date:** October 14, 2025  
**Status:** Production Ready

---

## ğŸ“‹ Overview

This folder contains **production-ready specifications and templates** for deploying the Informer time series forecasting model with SQL database integration.

### What's Included

| File | Purpose | Key Features |
|------|---------|--------------|
| `model_specs.py` | Complete model configuration system | 120+ parameters, type-safe, presets for common use cases |
| `config_matrix.csv` | Parameter reference guide | All parameters with definitions, scopes, and examples |
| `sql_data_pipeline.py` | SQL data extraction pipeline | Optimized queries, connection pooling, validation |
| `deployment_template.py` | End-to-end deployment template | Complete pipeline from data to API |
| `README.md` | This file | Documentation and usage guide |

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install torch pandas numpy sqlalchemy psycopg2-binary pyarrow scikit-learn
```

### 2. Set Environment Variables

```bash
export DB_HOST="your-database-host.com"
export DB_PORT="5432"
export DB_NAME="timeseries_db"
export DB_USER="your_username"
export DB_PASSWORD="your_password"
export MLFLOW_URI="http://mlflow-server:5000"
```

### 3. Create Configuration

```python
from model_specs import ConfigPresets

# Start with a preset
config = ConfigPresets.energy_forecasting()

# Customize for your use case
config.data_source.host = "your-db-host.com"
config.data_schema.table_name = "your_table"
config.data_schema.target_column = "your_target"

# Save configuration
config.to_json("my_config.json")
```

### 4. Extract Data from SQL

```python
from sql_data_pipeline import SQLTimeSeriesPipeline

# Create pipeline
pipeline = SQLTimeSeriesPipeline(config)

# Extract data
df = pipeline.run()

# Save for faster iteration
pipeline.save_to_parquet("data/extracted_data.parquet")
```

### 5. Run Complete Pipeline

```bash
python deployment_template.py full-pipeline --config my_config.json
```

---

## ğŸ“Š Configuration System

### Configuration Hierarchy

```
InformerConfig (Master)
â”œâ”€â”€ DataSourceConfig (Database connection)
â”œâ”€â”€ DataTableSchema (Table structure)
â”œâ”€â”€ DataFilterConfig (Query filters)
â”œâ”€â”€ DataPreprocessingConfig (Cleaning)
â”œâ”€â”€ NormalizationConfig (Scaling)
â”œâ”€â”€ InformerArchitectureConfig (Model architecture)
â”œâ”€â”€ OptimizationConfig (Training optimization)
â”œâ”€â”€ TrainingConfig (Training loop)
â”œâ”€â”€ DataSplitConfig (Train/val/test split)
â”œâ”€â”€ InferenceConfig (Prediction settings)
â”œâ”€â”€ DeploymentConfig (Production deployment)
â””â”€â”€ ExperimentConfig (MLOps tracking)
```

### Available Presets

| Preset | Use Case | seq_len | pred_len | Frequency |
|--------|----------|---------|----------|-----------|
| `small_dataset()` | <100K samples | 96 | 24 | Any |
| `medium_dataset()` | 100K-1M samples | 96 | 24 | Any |
| `large_dataset()` | >1M samples | 96 | 24 | Any |
| `energy_forecasting()` | Electricity load | 168 | 24 | Hourly |
| `financial_forecasting()` | Stock prices | 252 | 20 | Daily |
| `weather_forecasting()` | Temperature | 144 | 48 | 10-min |
| `traffic_forecasting()` | Traffic flow | 96 | 12 | 15-min |

### Example: Custom Configuration

```python
from model_specs import InformerConfig

# Start from scratch
config = InformerConfig()

# Configure database
config.data_source.database_type = "postgresql"
config.data_source.host = "prod-db.company.com"
config.data_source.database = "analytics"

# Configure data
config.data_schema.table_name = "sensor_readings"
config.data_schema.timestamp_column = "measured_at"
config.data_schema.target_column = "temperature"
config.data_schema.value_columns = ["temp", "humidity", "pressure"]

# Configure model
config.architecture.seq_len = 168  # 1 week lookback
config.architecture.pred_len = 24  # 1 day forecast
config.architecture.d_model = 512
config.architecture.n_heads = 8

# Configure training
config.training.batch_size = 32
config.training.num_epochs = 10
config.training.use_gpu = True

# Validate and save
config.validate()
config.to_json("sensor_config.json")
```

---

## ğŸ—„ï¸ SQL Database Integration

### Supported Databases

- âœ… PostgreSQL
- âœ… MySQL
- âœ… Google BigQuery
- âœ… Snowflake

### Connection Configuration

```python
from model_specs import DataSourceConfig

db_config = DataSourceConfig(
    database_type="postgresql",
    host="your-host.com",
    port=5432,
    database="timeseries_db",
    username="readonly_user",
    password="secure_password",
    use_ssl=True,
    connection_pool_size=5
)
```

### Query Optimization Features

1. **Automatic WHERE clause generation** from filters
2. **Connection pooling** for concurrent queries
3. **Chunk-based loading** for large datasets
4. **Time-based aggregation** for downsampling
5. **Window functions** for feature engineering
6. **Multi-entity support** for batch forecasting

### Example: Extract Data with Filters

```python
from model_specs import InformerConfig
from sql_data_pipeline import SQLTimeSeriesPipeline

config = InformerConfig()

# Configure filters
config.data_filter.start_date = "2023-01-01"
config.data_filter.end_date = "2024-01-01"
config.data_filter.exclude_nulls = True
config.data_filter.min_value = 0.0
config.data_filter.max_value = 1000.0

# Extract
pipeline = SQLTimeSeriesPipeline(config)
df = pipeline.run()
```

### Example: Multi-Entity Forecasting

```python
# Configure for multiple sensors/locations
config.data_schema.entity_column = "sensor_id"
config.data_filter.entity_ids = ["SENSOR_001", "SENSOR_002", "SENSOR_003"]

# Extract all entities
pipeline = SQLTimeSeriesPipeline(config)
df = pipeline.run()

# Or extract one entity at a time
for entity_id in config.data_filter.entity_ids:
    entity_df = pipeline.extractor.extract_by_entity(entity_id)
    # Process entity_df...
```

---

## ğŸ”§ Configuration Reference

### Critical Parameters

| Parameter | Location | Default | Description |
|-----------|----------|---------|-------------|
| `seq_len` | architecture | 96 | Input sequence length (lookback window) |
| `pred_len` | architecture | 24 | Prediction horizon (forecast length) |
| `d_model` | architecture | 512 | Model embedding dimension |
| `n_heads` | architecture | 8 | Number of attention heads |
| `e_layers` | architecture | 2 | Number of encoder layers |
| `batch_size` | training | 32 | Training batch size |
| `learning_rate` | optimization | 1e-4 | Initial learning rate |

### Data Configuration

```python
# Table schema
config.data_schema.table_name = "time_series_data"
config.data_schema.timestamp_column = "timestamp"
config.data_schema.target_column = "target"
config.data_schema.value_columns = ["feature_1", "feature_2", ...]

# Filters
config.data_filter.start_date = "2023-01-01"
config.data_filter.end_date = "2024-01-01"
config.data_filter.entity_ids = ["ID1", "ID2"]  # Optional

# Preprocessing
config.preprocessing.missing_value_strategy = "forward_fill"
config.preprocessing.outlier_treatment = "clip"
config.preprocessing.create_time_features = True

# Normalization
config.normalization.method = "standard"  # standard, minmax, robust
config.normalization.per_feature = True
config.normalization.fit_on_train_only = True  # CRITICAL!
```

### Model Architecture

```python
# Input/Output
config.architecture.enc_in = 7   # Number of input features
config.architecture.dec_in = 7   # Decoder input features
config.architecture.c_out = 1    # Number of outputs

# Sequence lengths
config.architecture.seq_len = 96      # Lookback
config.architecture.label_len = 48    # Start token
config.architecture.pred_len = 24     # Forecast

# Model size
config.architecture.d_model = 512     # Model dimension
config.architecture.d_ff = 2048       # FFN dimension
config.architecture.n_heads = 8       # Attention heads

# Layers
config.architecture.e_layers = 2      # Encoder layers
config.architecture.d_layers = 1      # Decoder layers

# ProbSparse attention
config.architecture.factor = 5        # Sampling factor
config.architecture.distil = True     # Use distilling
```

### Training Configuration

```python
# Training loop
config.training.batch_size = 32
config.training.num_epochs = 10
config.training.use_early_stopping = True
config.training.patience = 3

# Hardware
config.training.use_gpu = True
config.training.gpu_ids = [0, 1, 2]
config.training.use_multi_gpu = True
config.training.num_workers = 4

# Checkpointing
config.training.save_checkpoints = True
config.training.checkpoint_dir = "./checkpoints/"
config.training.save_best_only = True

# Optimization
config.optimization.optimizer = "adam"
config.optimization.learning_rate = 1e-4
config.optimization.use_lr_scheduler = True
config.optimization.use_mixed_precision = True
config.optimization.use_grad_clip = True
```

---

## ğŸ“ˆ Usage Examples

### Example 1: Energy Load Forecasting

```python
from model_specs import ConfigPresets
from sql_data_pipeline import SQLTimeSeriesPipeline

# Use energy preset
config = ConfigPresets.energy_forecasting()

# Customize
config.data_source.host = "energy-db.company.com"
config.data_schema.table_name = "grid_loads"
config.data_schema.target_column = "load_mw"
config.data_schema.value_columns = [
    "temperature", "humidity", "hour_of_day", 
    "day_of_week", "is_holiday", "load_mw"
]

# Extract and preprocess
pipeline = SQLTimeSeriesPipeline(config)
df = pipeline.run()

# Train model (implement in deployment_template.py)
# train_model(df, config)
```

### Example 2: Stock Price Prediction

```python
config = ConfigPresets.financial_forecasting()

config.data_schema.table_name = "stock_ohlcv"
config.data_schema.target_column = "close_price"
config.data_schema.value_columns = [
    "open", "high", "low", "close", "volume", "vwap"
]

# Financial data often has outliers - use robust scaling
config.normalization.method = "robust"

# Extract data
pipeline = SQLTimeSeriesPipeline(config)
df = pipeline.run()
```

### Example 3: Weather Forecasting

```python
config = ConfigPresets.weather_forecasting()

config.data_schema.table_name = "weather_stations"
config.data_schema.target_column = "temperature"
config.data_schema.value_columns = [
    "temperature", "humidity", "pressure", 
    "wind_speed", "precipitation"
]

# 10-minute data
config.architecture.freq = "t"

# Multi-station forecasting
config.data_schema.entity_column = "station_id"
config.data_filter.entity_ids = ["STATION_001", "STATION_002"]

pipeline = SQLTimeSeriesPipeline(config)
df = pipeline.run()
```

### Example 4: Traffic Flow Prediction

```python
config = ConfigPresets.traffic_forecasting()

config.data_schema.table_name = "traffic_sensors"
config.data_schema.target_column = "flow_rate"
config.data_schema.value_columns = [
    "flow_rate", "occupancy", "speed", "is_rush_hour"
]

# 15-minute intervals
config.architecture.freq = "t"

# Configure for real-time inference
config.inference.inference_batch_size = 1
config.deployment.enable_monitoring = True

pipeline = SQLTimeSeriesPipeline(config)
df = pipeline.run()
```

---

## ğŸ—ï¸ Architecture Specifications

### Model Sizes

| Size | d_model | d_ff | n_heads | e_layers | Parameters | Use Case |
|------|---------|------|---------|----------|------------|----------|
| Small | 256 | 1024 | 4 | 2 | ~5M | Testing, small datasets |
| Medium | 512 | 2048 | 8 | 2 | ~20M | Most production use cases |
| Large | 512 | 2048 | 8 | 3 | ~30M | Complex patterns, large data |
| XLarge | 768 | 3072 | 12 | 4 | ~60M | Research, maximum accuracy |

### Complexity Analysis

**Vanilla Transformer:**
- Time: O(LÂ² Â· d)
- Space: O(LÂ² Â· E)

**Informer:**
- Time: O(L log L Â· d)
- Space: O(L (log L + E))

**Example:** For L=512, d=512
- Vanilla: ~134M operations
- Informer: ~23M operations
- **Speedup: ~5.8x**

### Memory Requirements

| seq_len | batch_size | d_model | GPU Memory (GB) |
|---------|------------|---------|-----------------|
| 96 | 32 | 512 | ~2.5 |
| 168 | 32 | 512 | ~3.5 |
| 336 | 32 | 512 | ~5.0 |
| 512 | 32 | 512 | ~6.5 |
| 1024 | 16 | 512 | ~8.0 |
| 2048 | 8 | 512 | ~10.5 |

---

## ğŸ“¦ Deployment

### Docker Deployment

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "deployment_template.py", "deploy", "--config", "production_config.json"]
```

### Kubernetes Deployment

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: informer-forecasting
spec:
  replicas: 3
  selector:
    matchLabels:
      app: informer
  template:
    metadata:
      labels:
        app: informer
    spec:
      containers:
      - name: informer
        image: your-registry/informer:latest
        ports:
        - containerPort: 8000
        env:
        - name: DB_HOST
          valueFrom:
            secretKeyRef:
              name: db-secrets
              key: host
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
            nvidia.com/gpu: "1"
          limits:
            memory: "8Gi"
            cpu: "4000m"
            nvidia.com/gpu: "1"
```

### API Server Template

```python
from fastapi import FastAPI
from pydantic import BaseModel
import torch

app = FastAPI(title="Informer Forecasting API")

# Load model at startup
model = None
scaler = None

@app.on_event("startup")
async def load_model():
    global model, scaler
    # Load model and scaler
    pass

class PredictionRequest(BaseModel):
    entity_id: str
    timestamp: str
    features: List[float]

@app.post("/predict")
async def predict(request: PredictionRequest):
    # Preprocess
    # Inference
    # Postprocess
    return {"predictions": predictions.tolist()}

@app.get("/health")
async def health():
    return {"status": "healthy"}
```

---

## ğŸ§ª Testing

### Unit Tests

```python
# test_config.py
def test_config_validation():
    config = InformerConfig()
    config.architecture.seq_len = 96
    config.architecture.pred_len = 24
    config.validate()  # Should pass

def test_config_presets():
    for preset_name in ['small', 'medium', 'large']:
        config = create_config_from_preset(preset_name)
        assert config.architecture.d_model > 0
```

### Integration Tests

```python
# test_pipeline.py
def test_sql_extraction():
    config = ConfigPresets.energy_forecasting()
    pipeline = SQLTimeSeriesPipeline(config)
    df = pipeline.run()
    assert len(df) > 0
    assert config.data_schema.target_column in df.columns
```

---

## ğŸ“Š Monitoring

### Metrics to Track

**Training:**
- Loss (MSE/MAE)
- Validation metrics
- Learning rate
- Gradient norms
- Training time per epoch

**Inference:**
- Latency (p50, p95, p99)
- Throughput (predictions/sec)
- Error rate
- Cache hit rate

**Data Quality:**
- Missing value percentage
- Outlier count
- Distribution shift
- Data freshness

### Logging Configuration

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('informer.log'),
        logging.StreamHandler()
    ]
)
```

---

## ğŸ”’ Security Best Practices

1. **Never hardcode credentials** - use environment variables
2. **Use SSL/TLS** for database connections
3. **Implement API authentication** (JWT, OAuth)
4. **Rate limiting** on API endpoints
5. **Input validation** on all user inputs
6. **Audit logging** for predictions
7. **Model versioning** for rollback capability
8. **Regular security updates** for dependencies

---

## ğŸ“š Additional Resources

### Documentation
- **MATH_BREAKDOWN.txt** - Mathematical foundations
- **RESEARCH_BREAKDOWN.txt** - Research insights and applications
- **IMPLEMENTATION_SPEC.md** - Detailed implementation guide
- **DATA_ENGINEERING_SPEC.md** - Data preprocessing guide

### Papers
- Informer: [arXiv:2012.07436](https://arxiv.org/abs/2012.07436)
- Attention Is All You Need: [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

### Code Repositories
- Official Informer: https://github.com/zhouhaoyi/Informer2020
- This implementation: [Your repository]

---

## ğŸ¤ Support

For questions or issues:

1. Check `config_matrix.csv` for parameter definitions
2. Review example code in this README
3. See `deployment_template.py` for complete pipeline
4. Contact: engineering@your-company.com

---

## ğŸ“ License

Copyright (c) 2025 Your Company. All rights reserved.

---

## ğŸ”„ Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0.0 | 2025-10-14 | Initial release |

---

**Built with â¤ï¸ by the ML Engineering Team**

