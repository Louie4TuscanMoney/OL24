# Dejavu Implementation Specification

**Data-Centric Time Series Forecasting via Cross-Similarity**

**Paper Source:** Kang et al., "D√©j√† vu: A data-centric forecasting approach through time series cross-similarity", arXiv:1909.00221v3 [stat.ME], September 2020  
**Authors:** Yanfei Kang (Beihang U), Evangelos Spiliotis (NTUA), Fotios Petropoulos (U Bath), Nikolaos Athiniotis (NTUA), Feng Li (CUFE), Vassilios Assimakopoulos (NTUA)

**Date:** October 14, 2025  
**Model:** Dejavu - Cross-Similarity Pattern Matching (Model-Free)  
**Objective:** Production-ready implementation of data-centric forecasting without model training

**Paper-Verified:** Tested on M1/M3 (3,830 series) with M4 reference set (95,000 series)
- ‚úÖ Yearly: MASE 2.783 (BEST vs ETS/ARIMA/Theta/SHD)
- ‚úÖ Monthly: MASE 0.932 (tied with ETS)
- ‚úÖ ETS-Similarity combo: BEST across all frequencies

---

## üìö Documentation Structure

This folder contains complete specifications for implementing Dejavu:

| File | Purpose | Key Content |
|------|---------|-------------|
| **MATH_BREAKDOWN.txt** | Mathematical foundations | Similarity measures, K-NN, DTW, complexity analysis |
| **RESEARCH_BREAKDOWN.txt** | Research insights & applications | Use cases, deployment, interpretability |
| **DEJAVU_IMPLEMENTATION_SPEC.md** | This file - Implementation overview | Architecture, algorithms, deployment |
| **DATA_ENGINEERING_DEJAVU.md** | Data pipeline specifications | Pattern extraction, database management |

---

## üéØ Quick Navigation

### For Mathematical Details:
‚Üí See **MATH_BREAKDOWN.txt** for:
- Similarity measures (Euclidean, DTW, correlation)
- K-NN forecasting mathematics
- Weighting schemes
- Complexity analysis

### For Research & Applications:
‚Üí See **RESEARCH_BREAKDOWN.txt** for:
- Use cases (energy, retail, finance, weather)
- Deployment strategies
- Interpretability advantages
- When to use Dejavu vs. deep learning

### For Data Engineering:
‚Üí See **DATA_ENGINEERING_DEJAVU.md** for:
- Pattern extraction pipelines
- Database management
- Online learning
- SQL integration

---

## üöÄ Core Innovation: Data-Centric Forecasting

### Traditional Approach (Model-Centric)
```
Collect Data ‚Üí Train Model ‚Üí Make Predictions
                ‚Üë
         Hours to days
```

### Dejavu Approach (Data-Centric)
```
Query Pattern ‚Üí Find Similar Past ‚Üí Use Their Outcomes
                        ‚Üë
                  Instant (no training!)
```

**Key Advantage:** No training phase - instant deployment!

---

## üèóÔ∏è Architecture Overview

### 1. Pattern Database

**Structure:**
```python
database = [
    {
        'pattern': [x_t, x_{t+1}, ..., x_{t+h-1}],  # Historical pattern
        'outcome': [y_t, y_{t+1}, ..., y_{t+H-1}],   # What happened next
        'timestamp': datetime,
        'metadata': {...}  # External conditions
    },
    ...
]
```

**Size:** 1,000 to 100,000+ patterns

### 2. Similarity Computation

**Distance Measures:**
```python
# Euclidean (fast, amplitude-sensitive)
d_L2(x, y) = ‚àö(Œ£(x_i - y_i)¬≤)

# DTW (slow, handles phase shifts)
d_DTW(x, y) = min_path Œ£|x_i - y_j|

# Correlation (scale-invariant)
d_corr(x, y) = 1 - |corr(x, y)|
```

### 3. K-Nearest Neighbors

**Algorithm:**
```python
1. Normalize query pattern
2. Compute distances to all database patterns
3. Select K nearest neighbors
4. Weight by similarity
5. Return weighted average of outcomes
```

**Complexity:** O(n¬∑h) where n=database size, h=pattern length

### 4. Forecast Aggregation

**Weighted Average:**
```python
≈∑ = Œ£(w_i ¬∑ outcome_i) / Œ£(w_i)

where w_i = exp(-distance_i / œÉ)
```

---

## üíª Implementation Components

### Component 1: Pattern Extractor

```python
class PatternExtractor:
    """
    Extract (pattern, outcome) pairs from time series
    """
    def __init__(self, pattern_length=24, forecast_horizon=1):
        """
        Args:
            pattern_length: Length of historical pattern (h)
            forecast_horizon: Number of steps to forecast (H)
        """
        self.h = pattern_length
        self.H = forecast_horizon
    
    def extract_patterns(self, timeseries):
        """
        Extract all valid (pattern, outcome) pairs
        
        Returns:
            patterns: List of (pattern, outcome) tuples
        """
        patterns = []
        n = len(timeseries)
        
        for t in range(n - self.h - self.H + 1):
            pattern = timeseries[t : t + self.h]
            outcome = timeseries[t + self.h : t + self.h + self.H]
            
            patterns.append({
                'pattern': np.array(pattern),
                'outcome': np.array(outcome),
                'timestamp': t
            })
        
        return patterns
```

### Component 2: Similarity Engine

```python
class SimilarityEngine:
    """
    Compute similarity between patterns
    """
    def __init__(self, method='euclidean', normalize=True):
        """
        Args:
            method: 'euclidean', 'dtw', 'correlation', 'cosine'
            normalize: Whether to z-score normalize patterns
        """
        self.method = method
        self.normalize = normalize
    
    def compute_distance(self, pattern1, pattern2):
        """
        Compute distance between two patterns
        """
        if self.normalize:
            pattern1 = self._normalize(pattern1)
            pattern2 = self._normalize(pattern2)
        
        if self.method == 'euclidean':
            return np.sqrt(np.sum((pattern1 - pattern2) ** 2))
        
        elif self.method == 'manhattan':
            return np.sum(np.abs(pattern1 - pattern2))
        
        elif self.method == 'correlation':
            corr = np.corrcoef(pattern1, pattern2)[0, 1]
            return 1 - abs(corr)
        
        elif self.method == 'cosine':
            dot = np.dot(pattern1, pattern2)
            norm = np.linalg.norm(pattern1) * np.linalg.norm(pattern2)
            return 1 - (dot / (norm + 1e-10))
        
        elif self.method == 'dtw':
            return self._compute_dtw(pattern1, pattern2)
    
    def _normalize(self, pattern):
        """Z-score normalization"""
        mean = np.mean(pattern)
        std = np.std(pattern)
        return (pattern - mean) / (std + 1e-10)
    
    def _compute_dtw(self, x, y):
        """Dynamic Time Warping distance"""
        n, m = len(x), len(y)
        dtw_matrix = np.full((n+1, m+1), np.inf)
        dtw_matrix[0, 0] = 0
        
        for i in range(1, n+1):
            for j in range(1, m+1):
                cost = abs(x[i-1] - y[j-1])
                dtw_matrix[i, j] = cost + min(
                    dtw_matrix[i-1, j],    # insertion
                    dtw_matrix[i, j-1],    # deletion
                    dtw_matrix[i-1, j-1]   # match
                )
        
        return dtw_matrix[n, m]
```

### Component 3: K-NN Forecaster

```python
class DejavuForecaster:
    """
    Main Dejavu forecasting class
    """
    def __init__(
        self,
        K=10,
        similarity_method='euclidean',
        weighting='gaussian',
        sigma=1.0
    ):
        """
        Args:
            K: Number of nearest neighbors
            similarity_method: Distance metric
            weighting: 'uniform', 'inverse', 'gaussian'
            sigma: Bandwidth for Gaussian weights
        """
        self.K = K
        self.database = []
        self.similarity_engine = SimilarityEngine(similarity_method)
        self.weighting = weighting
        self.sigma = sigma
    
    def fit(self, timeseries, pattern_length=24, forecast_horizon=1):
        """
        Build pattern database from historical data
        
        Args:
            timeseries: Historical time series
            pattern_length: Length of patterns to extract
            forecast_horizon: Forecast horizon
        """
        extractor = PatternExtractor(pattern_length, forecast_horizon)
        self.database = extractor.extract_patterns(timeseries)
        
        print(f"Database created: {len(self.database)} patterns")
    
    def predict(self, query_pattern):
        """
        Forecast by finding similar patterns
        
        Args:
            query_pattern: Recent observations (length = pattern_length)
        
        Returns:
            forecast: Predicted values
            neighbors: K nearest neighbors (for interpretability)
        """
        # Compute distances to all database patterns
        distances = []
        for entry in self.database:
            dist = self.similarity_engine.compute_distance(
                query_pattern,
                entry['pattern']
            )
            distances.append(dist)
        
        # Select K nearest neighbors
        nearest_indices = np.argsort(distances)[:self.K]
        neighbors = [self.database[i] for i in nearest_indices]
        neighbor_distances = [distances[i] for i in nearest_indices]
        
        # Compute weights
        weights = self._compute_weights(neighbor_distances)
        
        # Weighted average of outcomes
        outcomes = np.array([n['outcome'] for n in neighbors])
        forecast = np.average(outcomes, axis=0, weights=weights)
        
        return forecast, neighbors
    
    def _compute_weights(self, distances):
        """Compute weights from distances"""
        distances = np.array(distances)
        
        if self.weighting == 'uniform':
            weights = np.ones(len(distances)) / len(distances)
        
        elif self.weighting == 'inverse':
            weights = 1.0 / (distances + 1e-10)
            weights /= weights.sum()
        
        elif self.weighting == 'gaussian':
            weights = np.exp(-distances**2 / (2 * self.sigma**2))
            weights /= weights.sum()
        
        return weights
    
    def update(self, new_pattern, new_outcome):
        """
        Add new observation to database (online learning)
        """
        self.database.append({
            'pattern': new_pattern,
            'outcome': new_outcome,
            'timestamp': len(self.database)
        })
    
    def prune(self, max_size=10000):
        """
        Maintain database size (sliding window)
        """
        if len(self.database) > max_size:
            self.database = self.database[-max_size:]
```

---

## üìä Configuration & Hyperparameters

### Critical Parameters

```python
config = {
    # Pattern extraction
    'pattern_length': 24,      # Hours, days, etc. (h)
    'forecast_horizon': 1,     # Steps ahead (H)
    
    # K-NN
    'K': 10,                   # Number of neighbors
    'similarity_method': 'euclidean',  # euclidean, dtw, correlation
    
    # Weighting
    'weighting': 'gaussian',   # uniform, inverse, gaussian
    'sigma': 1.0,              # Bandwidth for Gaussian
    
    # Database
    'max_database_size': 10000,  # Sliding window
    'update_frequency': 'daily',  # How often to add new patterns
    
    # Normalization
    'normalize_patterns': True,  # Z-score normalization
}
```

### Domain-Specific Configurations

**Energy Forecasting:**
```python
energy_config = {
    'pattern_length': 24,      # Daily pattern (hourly data)
    'forecast_horizon': 24,    # 1 day ahead
    'K': 15,
    'similarity_method': 'euclidean',
    'max_database_size': 730 * 24  # 2 years hourly
}
```

**Retail Demand:**
```python
retail_config = {
    'pattern_length': 7,       # Weekly pattern (daily data)
    'forecast_horizon': 7,     # 1 week ahead
    'K': 10,
    'similarity_method': 'dtw',  # Handles timing variations
    'max_database_size': 1095    # 3 years daily
}
```

**Financial:**
```python
finance_config = {
    'pattern_length': 20,      # Trading pattern
    'forecast_horizon': 1,     # 1 step ahead
    'K': 5,                    # Fewer for noisy data
    'similarity_method': 'correlation',
    'max_database_size': 2520  # ~10 years daily
}
```

---

## üöÄ Quick Start Guide

### Step 1: Install Dependencies

```bash
pip install numpy pandas scikit-learn dtaidistance
```

### Step 2: Load Data

```python
import pandas as pd
import numpy as np

# Load time series
df = pd.read_csv('data.csv')
timeseries = df['value'].values
```

### Step 3: Create Forecaster

```python
from dejavu_forecaster import DejavuForecaster

# Initialize
forecaster = DejavuForecaster(
    K=10,
    similarity_method='euclidean',
    weighting='gaussian'
)

# Build database from historical data
forecaster.fit(
    timeseries[:-100],  # Use most data for database
    pattern_length=24,
    forecast_horizon=1
)
```

### Step 4: Make Predictions

```python
# Query: most recent 24 observations
query_pattern = timeseries[-24:]

# Forecast
forecast, neighbors = forecaster.predict(query_pattern)

print(f"Forecast: {forecast}")
print(f"Based on {len(neighbors)} similar past patterns")

# Show matched patterns for interpretability
for i, neighbor in enumerate(neighbors[:3]):
    print(f"Neighbor {i+1}: timestamp={neighbor['timestamp']}")
```

### Step 5: Evaluate

```python
# Walk-forward validation
test_period = timeseries[-100:]
forecasts = []

for t in range(len(test_period) - 24):
    query = test_period[t:t+24]
    forecast, _ = forecaster.predict(query)
    forecasts.append(forecast[0])

# Calculate error
mae = np.mean(np.abs(forecasts - test_period[24:]))
print(f"Test MAE: {mae}")
```

---

## üìà Performance Optimization

### 1. Fast K-NN Search

**For large databases (>10,000 patterns):**

```python
from sklearn.neighbors import NearestNeighbors

class FastDejavuForecaster(DejavuForecaster):
    """Optimized with scikit-learn's K-NN"""
    
    def fit(self, timeseries, pattern_length=24, forecast_horizon=1):
        super().fit(timeseries, pattern_length, forecast_horizon)
        
        # Build KNN index
        patterns = np.array([d['pattern'] for d in self.database])
        self.knn_model = NearestNeighbors(
            n_neighbors=self.K,
            metric='euclidean'
        )
        self.knn_model.fit(patterns)
    
    def predict(self, query_pattern):
        # Fast K-NN search
        distances, indices = self.knn_model.kneighbors(
            query_pattern.reshape(1, -1)
        )
        
        # Get neighbors
        neighbors = [self.database[i] for i in indices[0]]
        weights = self._compute_weights(distances[0])
        
        # Forecast
        outcomes = np.array([n['outcome'] for n in neighbors])
        forecast = np.average(outcomes, axis=0, weights=weights)
        
        return forecast, neighbors
```

**Speedup:** 10-100x faster for large databases

### 2. Approximate K-NN (FAISS)

**For very large databases (>1M patterns):**

```python
import faiss

class ScalableDejavuForecaster:
    """Use FAISS for billion-scale pattern matching"""
    
    def fit(self, timeseries, pattern_length=24, forecast_horizon=1):
        # Extract patterns
        extractor = PatternExtractor(pattern_length, forecast_horizon)
        self.database = extractor.extract_patterns(timeseries)
        
        # Build FAISS index
        patterns = np.array([d['pattern'] for d in self.database]).astype('float32')
        
        d = patterns.shape[1]  # Dimension
        self.index = faiss.IndexFlatL2(d)  # L2 distance
        self.index.add(patterns)
        
        print(f"FAISS index built: {self.index.ntotal} patterns")
    
    def predict(self, query_pattern, K=10):
        # Fast approximate K-NN
        query = query_pattern.reshape(1, -1).astype('float32')
        distances, indices = self.index.search(query, K)
        
        # Rest same as before...
```

**Speedup:** 1000x+ faster, scales to billions

---

## üîó Integration Patterns

### Pattern 1: Standalone Dejavu

```
SQL Data ‚Üí Pattern Extraction ‚Üí Dejavu Database ‚Üí
‚Üí Query Matching ‚Üí Forecast + Matched Patterns
```

**Use When:** Simple deployment, interpretability key

### Pattern 2: Dejavu + Conformal

```
SQL Data ‚Üí Dejavu (Point Forecast) ‚Üí
‚Üí Conformal Calibration ‚Üí Forecast + Uncertainty Intervals
```

**Use When:** Need statistical guarantees

### Pattern 3: Ensemble with Deep Learning

```
Query ‚Üí Dejavu        ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚Üí Informer/LSTM ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚Üí Weighted Average ‚Üí Forecast
     ‚Üí XGBoost       ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Use When:** Maximum accuracy needed

### Pattern 4: Hybrid (Dejavu + Trend Model)

```
Query ‚Üí ARIMA (Trend) ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚Üí Dejavu (Pattern) ‚îÄ‚îÄ‚îº‚Üí Combine ‚Üí Forecast
```

**Use When:** Data has both trend and patterns

---

## üìä Monitoring & Debugging

### Key Metrics

```python
class DejavuMonitor:
    """Monitor Dejavu performance"""
    
    def __init__(self):
        self.metrics = {
            'forecasts': [],
            'actuals': [],
            'neighbor_distances': [],
            'neighbor_variances': []
        }
    
    def log_forecast(self, forecast, neighbors, actual=None):
        """Log forecast for monitoring"""
        self.metrics['forecasts'].append(forecast)
        
        if actual is not None:
            self.metrics['actuals'].append(actual)
        
        # Neighbor statistics
        distances = [n['distance'] for n in neighbors]
        outcomes = [n['outcome'] for n in neighbors]
        
        self.metrics['neighbor_distances'].append(np.mean(distances))
        self.metrics['neighbor_variances'].append(np.var(outcomes))
    
    def get_health_metrics(self):
        """Check system health"""
        if len(self.metrics['actuals']) > 0:
            errors = np.abs(
                np.array(self.metrics['forecasts']) -
                np.array(self.metrics['actuals'])
            )
            mae = np.mean(errors)
        else:
            mae = None
        
        return {
            'mae': mae,
            'avg_neighbor_distance': np.mean(self.metrics['neighbor_distances']),
            'avg_outcome_variance': np.mean(self.metrics['neighbor_variances']),
            'n_forecasts': len(self.metrics['forecasts'])
        }
```

### Alert Conditions

- **High neighbor distance:** Queries finding poor matches ‚Üí need more data
- **High outcome variance:** Neighbors have very different outcomes ‚Üí ambiguous pattern
- **Increasing MAE:** Forecast quality degrading ‚Üí database outdated or drift

---

## üéØ Production Deployment

### Complete Pipeline

```python
def deploy_dejavu_pipeline(config):
    """
    End-to-end Dejavu deployment
    """
    # 1. Load historical data
    df = load_data_from_sql(config['data_source'])
    timeseries = df[config['target_column']].values
    
    # 2. Create forecaster
    forecaster = DejavuForecaster(
        K=config['K'],
        similarity_method=config['similarity_method'],
        weighting=config['weighting']
    )
    
    # 3. Build database
    forecaster.fit(
        timeseries,
        pattern_length=config['pattern_length'],
        forecast_horizon=config['forecast_horizon']
    )
    
    # 4. Save forecaster
    import joblib
    joblib.dump(forecaster, 'dejavu_forecaster.pkl')
    
    # 5. Deploy API
    deploy_api(forecaster, config)
    
    print("‚úì Dejavu deployed successfully!")
```

### API Endpoint (FastAPI)

```python
from fastapi import FastAPI
import joblib

app = FastAPI(title="Dejavu Forecasting API")

# Load forecaster at startup
forecaster = joblib.load('dejavu_forecaster.pkl')

@app.post("/forecast")
async def forecast(request: ForecastRequest):
    """
    Make forecast with pattern matching
    """
    query_pattern = np.array(request.query_pattern)
    
    # Forecast
    forecast, neighbors = forecaster.predict(query_pattern)
    
    # Return with matched patterns for interpretability
    return {
        'forecast': forecast.tolist(),
        'matched_patterns': [
            {
                'timestamp': n['timestamp'],
                'similarity': compute_similarity(query_pattern, n['pattern'])
            }
            for n in neighbors[:5]  # Top 5 for interpretability
        ],
        'confidence': compute_confidence(neighbors)
    }

@app.post("/update")
async def update(pattern: List[float], outcome: List[float]):
    """
    Update database with new observation
    """
    forecaster.update(np.array(pattern), np.array(outcome))
    return {"status": "updated"}
```

---

## üìù Summary

**Dejavu Key Features:**
- ‚úÖ **Instant deployment** - no training time
- ‚úÖ **Interpretable** - shows matched patterns
- ‚úÖ **Adaptive** - continuous database updates
- ‚úÖ **Simple** - just K-NN on patterns
- ‚úÖ **Transferable** - same algorithm across domains

**Ideal For:**
- Fast deployment needs
- Limited training data (<10K samples)
- Interpretability critical
- Non-stationary environments
- Pattern-rich domains

**Complete File Structure:**
```
Dejavu/
‚îú‚îÄ‚îÄ Dejavu...pdf                    ‚Üê Original paper
‚îú‚îÄ‚îÄ MATH_BREAKDOWN.txt              ‚Üê Mathematical foundations
‚îú‚îÄ‚îÄ RESEARCH_BREAKDOWN.txt          ‚Üê Applications & insights
‚îú‚îÄ‚îÄ DEJAVU_IMPLEMENTATION_SPEC.md   ‚Üê This file
‚îî‚îÄ‚îÄ DATA_ENGINEERING_DEJAVU.md      ‚Üê Data pipeline specs
```

**Next Steps:**
1. Read MATH_BREAKDOWN.txt for algorithms
2. Read RESEARCH_BREAKDOWN.txt for use cases
3. See DATA_ENGINEERING_DEJAVU.md for data pipeline
4. Implement using code templates above
5. Deploy with interpretable forecasts!

---

**Ready for instant, interpretable time series forecasting!** üéØ

*Version 1.0.0 - October 14, 2025*

